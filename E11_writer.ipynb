{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 도구 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['The Cat in the Hat', 'By Dr. Seuss', 'The sun did not shine.']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel//lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가면 잘라내세요!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cat in the Hat\n",
      "By Dr. Seuss\n",
      "The sun did not shine.\n",
      "It was too wet to play.\n",
      "So we sat in the house\n",
      "All that cold cold wet day.\n",
      "I sat there with Sally.\n",
      "We sat there we two.\n",
      "And I said How I wish\n",
      "We had something to do!\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> the cat in the hat <end>',\n",
       " '<start> by dr . seuss <end>',\n",
       " '<start> the sun did not shine . <end>',\n",
       " '<start> it was too wet to play . <end>',\n",
       " '<start> so we sat in the house <end>',\n",
       " '<start> all that cold cold wet day . <end>',\n",
       " '<start> i sat there with sally . <end>',\n",
       " '<start> we sat there we two . <end>',\n",
       " '<start> and i said how i wish <end>',\n",
       " '<start> we had something to do ! <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "#    if len(preprocess_sentence(sentence)) <= 15: continue\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    6  903 ...    0    0    0]\n",
      " [   2  122 2584 ...    0    0    0]\n",
      " [   2    6  304 ...    0    0    0]\n",
      " ...\n",
      " [ 673   27    6 ...    6  189    3]\n",
      " [   2  673   27 ...    0    0    0]\n",
      " [   2  673   27 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f4e197483d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen = 15, padding='post')  \n",
    "#    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 길이 확인\n",
    "len(tensor[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    6  903   14    6 1350    3    0    0    0]\n",
      " [   2  122 2584   20    1    3    0    0    0    0]\n",
      " [   2    6  304  166   70  559   20    3    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    6  903   14    6 1350    3    0    0    0    0    0    0    0]\n",
      "[   6  903   14    6 1350    3    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <END>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "#데이터 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 설계\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-3.01581586e-05,  3.00796499e-04, -2.98221625e-04, ...,\n",
       "          1.12974871e-04,  3.33018863e-04,  2.06015466e-05],\n",
       "        [-1.33034060e-04,  7.64945522e-04, -7.71292776e-04, ...,\n",
       "          4.85848068e-06,  5.25960408e-04,  9.00514424e-05],\n",
       "        [-1.97639340e-04,  7.59615155e-04, -1.13645697e-03, ...,\n",
       "          2.51424663e-05,  6.45792112e-04, -1.66653859e-04],\n",
       "        ...,\n",
       "        [ 1.74648550e-04,  2.01610313e-03, -2.20868527e-03, ...,\n",
       "          2.62596970e-03, -3.76730575e-04,  2.08904912e-05],\n",
       "        [ 3.99907614e-04,  2.45456211e-03, -2.40178546e-03, ...,\n",
       "          3.33099905e-03, -1.00937916e-03,  7.47312224e-05],\n",
       "        [ 6.28689362e-04,  2.82754074e-03, -2.58201710e-03, ...,\n",
       "          3.96129908e-03, -1.65014458e-03,  1.55535075e-04]],\n",
       "\n",
       "       [[-3.01581586e-05,  3.00796499e-04, -2.98221625e-04, ...,\n",
       "          1.12974871e-04,  3.33018863e-04,  2.06015466e-05],\n",
       "        [-1.33034060e-04,  7.64945522e-04, -7.71292776e-04, ...,\n",
       "          4.85848068e-06,  5.25960408e-04,  9.00514424e-05],\n",
       "        [ 2.44841340e-05,  7.49797560e-04, -8.42702226e-04, ...,\n",
       "         -2.39337533e-04,  6.01465232e-04,  1.71025258e-05],\n",
       "        ...,\n",
       "        [ 1.59594568e-03,  2.29643937e-03, -1.92754227e-03, ...,\n",
       "          3.67326941e-03, -1.27460388e-03,  2.16103763e-05],\n",
       "        [ 1.67113764e-03,  2.70359707e-03, -2.21446622e-03, ...,\n",
       "          4.38428856e-03, -1.84054847e-03,  1.62340104e-04],\n",
       "        [ 1.72789348e-03,  3.04325134e-03, -2.47407518e-03, ...,\n",
       "          4.96659847e-03, -2.40036217e-03,  3.25440866e-04]],\n",
       "\n",
       "       [[-9.91925845e-05, -1.59625735e-04, -1.81031282e-05, ...,\n",
       "          1.70452637e-04,  4.06428626e-06, -2.00600261e-04],\n",
       "        [-6.62635954e-04, -2.45487725e-04, -1.55603135e-04, ...,\n",
       "          4.71970736e-04,  2.22592847e-04, -3.75945150e-04],\n",
       "        [-1.19177019e-03, -3.85205261e-04, -2.62563233e-04, ...,\n",
       "          7.24693004e-04,  2.99540348e-04, -9.05976398e-04],\n",
       "        ...,\n",
       "        [-2.60075345e-03,  9.09670081e-04, -1.71397274e-04, ...,\n",
       "         -3.30562820e-03, -1.82302669e-04, -2.01805704e-03],\n",
       "        [-2.73584900e-03,  7.14346301e-04, -3.12984630e-04, ...,\n",
       "         -3.41045391e-03, -2.89375486e-04, -1.80460105e-03],\n",
       "        [-2.73323944e-03,  7.63818854e-04, -4.03821352e-04, ...,\n",
       "         -3.54869966e-03, -1.94813110e-04, -2.01370870e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.01581586e-05,  3.00796499e-04, -2.98221625e-04, ...,\n",
       "          1.12974871e-04,  3.33018863e-04,  2.06015466e-05],\n",
       "        [-1.09384862e-04,  2.39930072e-04, -5.41419315e-04, ...,\n",
       "          3.07573879e-04,  4.20508353e-04,  3.44788510e-04],\n",
       "        [ 3.07601353e-04,  1.92498090e-04, -6.21272600e-04, ...,\n",
       "          2.88079318e-04,  2.60490080e-04,  5.16769534e-04],\n",
       "        ...,\n",
       "        [ 1.07205205e-03,  8.53594975e-04, -8.49099364e-04, ...,\n",
       "          2.30884342e-03, -1.11509755e-03, -4.85755649e-04],\n",
       "        [ 1.10502343e-03,  1.39906025e-03, -1.25534262e-03, ...,\n",
       "          3.11068143e-03, -1.47004961e-03, -4.24393977e-04],\n",
       "        [ 1.16901891e-03,  1.90881232e-03, -1.64655049e-03, ...,\n",
       "          3.84432939e-03, -1.89179776e-03, -3.03023728e-04]],\n",
       "\n",
       "       [[-3.01581586e-05,  3.00796499e-04, -2.98221625e-04, ...,\n",
       "          1.12974871e-04,  3.33018863e-04,  2.06015466e-05],\n",
       "        [-1.12898604e-04,  2.34840540e-04, -6.18361577e-04, ...,\n",
       "          1.57728573e-04,  5.07924939e-04, -2.15215026e-04],\n",
       "        [ 8.36336985e-05, -3.17478494e-04, -3.04994814e-04, ...,\n",
       "         -1.46549737e-05,  7.23714766e-05, -4.73008084e-04],\n",
       "        ...,\n",
       "        [ 8.10708676e-04, -1.55186222e-04, -2.60496134e-04, ...,\n",
       "          5.71238459e-04, -1.20261067e-03, -3.70801368e-04],\n",
       "        [ 9.69172339e-04,  4.47707105e-04, -6.69613015e-04, ...,\n",
       "          1.48787850e-03, -1.33389805e-03, -4.41351964e-04],\n",
       "        [ 1.13568001e-03,  1.04050804e-03, -1.09457772e-03, ...,\n",
       "          2.39901501e-03, -1.58154790e-03, -4.50587773e-04]],\n",
       "\n",
       "       [[-3.01581586e-05,  3.00796499e-04, -2.98221625e-04, ...,\n",
       "          1.12974871e-04,  3.33018863e-04,  2.06015466e-05],\n",
       "        [-3.93114547e-04,  4.15256742e-04, -1.27829524e-04, ...,\n",
       "          1.14762028e-04, -9.24472552e-05,  2.71651097e-05],\n",
       "        [-6.95874565e-04,  2.69461947e-04, -1.93518586e-04, ...,\n",
       "         -1.38165953e-04, -4.62394237e-04,  1.84510445e-05],\n",
       "        ...,\n",
       "        [-1.65026612e-03, -2.36887671e-03,  1.03875133e-03, ...,\n",
       "          1.20037957e-03, -1.05648371e-03,  7.03116893e-05],\n",
       "        [-1.84487284e-03, -2.51197303e-03,  1.15972536e-03, ...,\n",
       "          1.37027923e-03, -9.24260588e-04,  3.87820037e-05],\n",
       "        [-1.68142852e-03, -2.32954184e-03,  1.16149196e-03, ...,\n",
       "          1.27569062e-03, -8.39609536e-04, -1.23809032e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 33,133,793\n",
      "Trainable params: 33,133,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 104s 152ms/step - loss: 3.5606\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 105s 152ms/step - loss: 3.0675\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 104s 152ms/step - loss: 2.8628\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 104s 151ms/step - loss: 2.7010\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 105s 153ms/step - loss: 2.5567\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 105s 152ms/step - loss: 2.4226\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 105s 153ms/step - loss: 2.2950\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 104s 152ms/step - loss: 2.1737\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 104s 152ms/step - loss: 2.0570\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 105s 153ms/step - loss: 1.9442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4d94009810>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "256, 1024, loss: 2.2301\n",
    "512, 1024, loss: 1.9442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love the way you lie <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### 데이터 전처리\n",
    "이번 프로젝트에서 가장 난이도가 있던 부분은 데이터 전처리이다. 가장 애매한 부분은 어떤 문자는 남겨놓고 어떤 문자는 버릴지 결정하는 일이다. 예를 들어 특수문자 '('같은 경우 사용하는 경우는 드물지만 전혀 안 쓴다고 볼 수는 없으며 이번 프로젝트에서 알파벳를 비롯한 몇몇 특수문자를 제외하곤 제거했는데 résumé와 같이 외국어를 그대로 표기하는 경우 필요한 데이터가 날아갈 가능성이 높다.\n",
    "이는 한글에선 더 큰 문제인데 동음이의어나 잘 사용되지 않는 단어의 경우 주석(註釋)과 같은 형태로 한글 + 외국어의 형태로 표기하는 경우가 종종 존재하며 아예 영어와 병기하는 경우도 있기 때문이다. 이러한 이유로 입력 데이터의 종류와 성격에 따라서 데이터 전처리에 있어서 신중을 기해야 한다.\n",
    "### 모델 설계\n",
    "모델의 경우 자연어처리에 있어서 뛰어난 성능을 가졌다고 알려진 LSTM이 사용되었다.\n",
    "파라미터 설정의 경우 hidden size를 고정값으로 두고 embedding size는 각각 256, 512를 사용했으며 epoch는 10을 주었다.\n",
    "### 총평\n",
    "embedding size 256, hidden size 1024, epoch 10을 주었을때 loss는 2.23이었으며 i love를 입력 데이터로 주었을때 '<start> i love cease sunset emerald hairy lots lots reminisce reminisce reminisce hypnotized hypnotized fantastic fantastic fantastic forward kindly kindly '가 출력되었다.\n",
    "반면 embedding size 512, hidden size 1024, epoch 10을 주었을때 loss는 1.94였고 출력 데이터는 '<start> i love the way you lie <end> '였다.\n",
    "생각보다 훨씬 완성도가 높은 결과물이 나왔다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
