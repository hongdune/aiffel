{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 모든 단어 사용\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "2. 빈도수 상위 5,000개의 단어만 사용\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)\n",
    "사용할 모델 나이브 베이즈 분류기, CNB, 로지스틱 회귀, 서포트 벡터 머신, 결정 트리, 랜덤포레스트, 그래디언트 부스팅 트리, 보팅\n",
    "\n",
    "3. 딥러닝 모델과 비교해 보기\n",
    "위 과정을 통해 나온 최적의 모델과 단어수 조건에서, 본인이 선택한 다른 모델을 적용한 결과와 비교해 봅시다. 감정분석 등에 사용했던 RNN이나 1-D CNN 등의 딥러닝 모델 중 하나를 선택해서 오늘 사용했던 데이터셋을 학습해 보고 나오는 결과를 비교해 봅시다. 단, 공정한 비교를 위해 이때 Word2Vec 등의 pretrained model은 사용하지 않도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier #Logistic Regression\n",
    "from sklearn.naive_bayes import ComplementNB #CNB\n",
    "from sklearn.tree import DecisionTreeClassifier #결정나무\n",
    "from sklearn.ensemble import RandomForestClassifier #랜덤포레스트\n",
    "from sklearn.ensemble import GradientBoostingClassifier #GBT\n",
    "from sklearn.ensemble import VotingClassifier #보팅\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 단어 사용\n",
    "\n",
    "(x_train_A, y_train_A), (x_test_A, y_test_A) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "(x_train_B, y_train_B), (x_test_B, y_test_B) = reuters.load_data(num_words=5000, test_split=0.2)\n",
    "(x_train_C, y_train_C), (x_test_C, y_test_C) = reuters.load_data(num_words=10000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A클래스의 수 : 46\n",
      "B클래스의 수 : 46\n",
      "C클래스의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "num_classes_A = max(y_train_A) + 1\n",
    "num_classes_B = max(y_train_B) + 1\n",
    "num_classes_C = max(y_train_C) + 1\n",
    "print('A클래스의 수 : {}'.format(num_classes_A))\n",
    "print('B클래스의 수 : {}'.format(num_classes_B))\n",
    "print('C클래스의 수 : {}'.format(num_classes_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#데이터 확인하기\n",
    "\n",
    "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train_A)))\n",
    "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train_A))/len(x_train_A)))\n",
    "\n",
    "plt.hist([len(s) for s in x_train_A], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(12,5)\n",
    "sns.countplot(y_train_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label의 편향성이 확인된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "index_to_word = {index + 3 : word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_word[index] for index in x_train_A[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_A = []\n",
    "for i in range(len(x_train_A)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_A[i]])\n",
    "    decoded_A.append(t)\n",
    "\n",
    "x_train_A = decoded_A\n",
    "\n",
    "decoded_A_T = []\n",
    "for i in range(len(x_test_A)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_A[i]])\n",
    "    decoded_A_T.append(t)\n",
    "\n",
    "x_test_A = decoded_A_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_B = []\n",
    "for i in range(len(x_train_B)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_B[i]])\n",
    "    decoded_B.append(t)\n",
    "\n",
    "x_train_B = decoded_B\n",
    "\n",
    "decoded_B_T = []\n",
    "for i in range(len(x_test_B)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_B[i]])\n",
    "    decoded_B_T.append(t)\n",
    "\n",
    "x_test_B = decoded_B_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_C = []\n",
    "for i in range(len(x_train_C)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_C[i]])\n",
    "    decoded_C.append(t)\n",
    "\n",
    "x_train_C = decoded_C\n",
    "\n",
    "decoded_C_T = []\n",
    "for i in range(len(x_test_C)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_C[i]])\n",
    "    decoded_C_T.append(t)\n",
    "\n",
    "x_test_C = decoded_C_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 26506)\n"
     ]
    }
   ],
   "source": [
    "dtmvector_A = CountVectorizer()\n",
    "x_train_A_dtm = dtmvector_A.fit_transform(x_train_A)\n",
    "print(x_train_A_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 26506)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer_A = TfidfTransformer()\n",
    "tfidfv_A = tfidf_transformer_A.fit_transform(x_train_A_dtm)\n",
    "print(tfidfv_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867)\n"
     ]
    }
   ],
   "source": [
    "dtmvector_B = CountVectorizer()\n",
    "x_train_B_dtm = dtmvector_B.fit_transform(x_train_B)\n",
    "print(x_train_B_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer_B = TfidfTransformer()\n",
    "tfidfv_B = tfidf_transformer_B.fit_transform(x_train_B_dtm)\n",
    "print(tfidfv_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n"
     ]
    }
   ],
   "source": [
    "dtmvector_C = CountVectorizer()\n",
    "x_train_C_dtm = dtmvector_C.fit_transform(x_train_C)\n",
    "print(x_train_C_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer_C = TfidfTransformer()\n",
    "tfidfv_C = tfidf_transformer_C.fit_transform(x_train_C_dtm)\n",
    "print(tfidfv_C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나이브 베이즈 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 데이터의 accuracy: 0.5997328584149599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.79      0.21      0.33       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.72      0.92      0.81       813\n",
      "           4       0.45      0.96      0.61       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.80      0.29      0.42        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.75      0.18      0.29        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.73      0.58      0.64       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60      2246\n",
      "   macro avg       0.09      0.07      0.07      2246\n",
      "weighted avg       0.50      0.60      0.50      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mod_A = MultinomialNB()\n",
    "mod_A.fit(tfidfv_A, y_train_A)\n",
    "x_test_A_dtm = dtmvector_A.transform(x_test_A)\n",
    "tfidfv_A_test = tfidf_transformer_A.transform(x_test_A_dtm)\n",
    "\n",
    "print(\"A의 NB accuracy:\", accuracy_score(y_test_A, mod_A.predict(tfidfv_A_test)))\n",
    "print(classification_report(y_test_A, mod_A.predict(tfidfv_A_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B 데이터의 accuracy: 0.6731967943009796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.50      0.80      0.62       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.86      0.89      0.87       813\n",
      "           4       0.59      0.95      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.48      0.73      0.58        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.14      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.66      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.51      0.81      0.63       133\n",
      "          20       0.90      0.13      0.23        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.16      0.12      0.11      2246\n",
      "weighted avg       0.60      0.67      0.60      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mod_B = MultinomialNB()\n",
    "mod_B.fit(tfidfv_B, y_train_B)\n",
    "x_test_B_dtm = dtmvector_B.transform(x_test_B)\n",
    "tfidfv_B_test = tfidf_transformer_B.transform(x_test_B_dtm)\n",
    "\n",
    "print(\"B의 NB accuracy:\", accuracy_score(y_test_B, mod_B.predict(tfidfv_B_test)))\n",
    "print(classification_report(y_test_B, mod_B.predict(tfidfv_B_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 데이터의 accuracy: 0.6567230632235085\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.62      0.69      0.65       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.81      0.90      0.85       813\n",
      "           4       0.51      0.96      0.67       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.08      0.15        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.66      0.63      0.64        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.03      0.05        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.69      0.56      0.61        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.60      0.78      0.68       133\n",
      "          20       1.00      0.04      0.08        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66      2246\n",
      "   macro avg       0.17      0.10      0.10      2246\n",
      "weighted avg       0.59      0.66      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mod_C = MultinomialNB()\n",
    "mod_C.fit(tfidfv_C, y_train_C)\n",
    "x_test_C_dtm = dtmvector_C.transform(x_test_C)\n",
    "tfidfv_C_test = tfidf_transformer_C.transform(x_test_C_dtm)\n",
    "\n",
    "print(\"C의 NB accuracy:\", accuracy_score(y_test_C, mod_C.predict(tfidfv_C_test)))\n",
    "print(classification_report(y_test_C, mod_C.predict(tfidfv_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes accuracy\n",
    "\n",
    "A: 0.6\n",
    "B: 0.67\n",
    "C: 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컴플리트 나이브 베이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A의 CNB accuracy: 0.7649154051647373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.63      0.88      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.75      0.93      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.43      0.08      0.13        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.96      0.73      0.83        30\n",
      "          11       0.55      0.67      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.62      0.54      0.58        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.77      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.65      0.55      0.59        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.89      0.23      0.36        70\n",
      "          21       0.84      0.59      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.62      0.42      0.46      2246\n",
      "weighted avg       0.75      0.76      0.73      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb_A = ComplementNB()\n",
    "cb_A.fit(tfidfv_A, y_train_A)\n",
    "\n",
    "print(\"A의 CNB accuracy:\", accuracy_score(y_test_A, cb_A.predict(tfidfv_A_test))) #예측값과 실제값 비교\n",
    "print(classification_report(y_test_A, cb_A.predict(tfidfv_A_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B의 CNB accuracy: 0.7707034728406055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.86      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.74      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.57      0.21      0.31        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.54      0.76      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.56      0.80      0.66       133\n",
      "          20       0.79      0.33      0.46        70\n",
      "          21       0.78      0.67      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.77      0.81        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb_B = ComplementNB()\n",
    "cb_B.fit(tfidfv_B, y_train_B)\n",
    "\n",
    "print(\"B의 CNB accuracy:\", accuracy_score(y_test_B, cb_B.predict(tfidfv_B_test))) #예측값과 실제값 비교\n",
    "print(classification_report(y_test_B, cb_B.predict(tfidfv_B_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C의 CNB accuracy: 0.7707034728406055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.64      0.88      0.74       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.75      0.92      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.50      0.13      0.21        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.55      0.73      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.58      0.59      0.59        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.79      0.73        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.75      0.30      0.43        70\n",
      "          21       0.74      0.63      0.68        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.50      0.60        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.85      0.74      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.25      0.10      0.14        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.75      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb_C = ComplementNB()\n",
    "cb_C.fit(tfidfv_C, y_train_C)\n",
    "\n",
    "print(\"C의 CNB accuracy:\", accuracy_score(y_test_C, cb_C.predict(tfidfv_C_test))) #예측값과 실제값 비교\n",
    "print(classification_report(y_test_C, cb_C.predict(tfidfv_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complement Naive Bayes accuracy\n",
    "\n",
    "A: 0.76\n",
    "B: 0.77\n",
    "C: 0.77\n",
    "\n",
    "5000과 10000개 사이에서 유의미한 차이가 확인되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A의 로지스틱 회귀 accuracy: 0.813446126447017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.73      0.80      0.76       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.92      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.74      0.71        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.69      0.73      0.71        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.66      0.62      0.64        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.70      0.77      0.73        99\n",
      "          17       0.73      0.67      0.70        12\n",
      "          18       0.81      0.65      0.72        20\n",
      "          19       0.68      0.70      0.69       133\n",
      "          20       0.64      0.51      0.57        70\n",
      "          21       0.66      0.85      0.74        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.69      0.75      0.72        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.88      0.74      0.81        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.88      0.58      0.70        12\n",
      "          31       0.80      0.62      0.70        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.76      0.64      0.67      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr_A = LogisticRegression(C=10000, penalty='l2')\n",
    "lr_A.fit(tfidfv_A, y_train_A)\n",
    "print(\"A의 로지스틱 회귀 accuracy:\", accuracy_score(y_test_A, lr_A.predict(tfidfv_A_test)))\n",
    "print(classification_report(y_test_A, lr_A.predict(tfidfv_A_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B의 로지스틱 회귀 accuracy: 0.8058771148708815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.80      0.79       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.74      0.68        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.73      0.70        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.66      0.68      0.67       133\n",
      "          20       0.61      0.47      0.53        70\n",
      "          21       0.62      0.78      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.67      0.40      0.50        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.38      0.27      0.32        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.73      0.61      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr_B = LogisticRegression(C=10000, penalty='l2')\n",
    "lr_B.fit(tfidfv_B, y_train_B)\n",
    "print(\"B의 로지스틱 회귀 accuracy:\", accuracy_score(y_test_B, lr_B.predict(tfidfv_B_test)))\n",
    "print(classification_report(y_test_B, lr_B.predict(tfidfv_B_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C의 로지스틱 회귀 accuracy: 0.8076580587711487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.78      0.76       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.92      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.59      0.59      0.59        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.68      0.75      0.71        99\n",
      "          17       0.75      0.75      0.75        12\n",
      "          18       0.86      0.60      0.71        20\n",
      "          19       0.68      0.68      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.64      0.58      0.61        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.57      0.40      0.47        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.88      0.58      0.70        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.36      0.36      0.36        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.62      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr_C = LogisticRegression(C=10000, penalty='l2')\n",
    "lr_C.fit(tfidfv_C, y_train_C)\n",
    "print(\"C의 로지스틱 회귀 accuracy:\", accuracy_score(y_test_C, lr_C.predict(tfidfv_C_test)))\n",
    "print(classification_report(y_test_C, lr_C.predict(tfidfv_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression의 accuracy\n",
    "\n",
    "A: 0.81\n",
    "B: 0.81\n",
    "C: 0.81\n",
    "\n",
    "전반적으로 큰 차이는 없으나 A의 성능이 약간 (소수점 2자리이하) 우세하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A의 SVM accuracy: 0.782279608192342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.70      0.73      0.72       105\n",
      "           2       0.67      0.60      0.63        20\n",
      "           3       0.92      0.92      0.92       813\n",
      "           4       0.79      0.86      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.81      0.93      0.87        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.60      0.68      0.64        38\n",
      "           9       0.76      0.88      0.81        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.62      0.73      0.67        83\n",
      "          12       0.31      0.31      0.31        13\n",
      "          13       0.54      0.51      0.53        37\n",
      "          14       0.33      0.50      0.40         2\n",
      "          15       0.60      0.33      0.43         9\n",
      "          16       0.67      0.70      0.68        99\n",
      "          17       0.67      0.33      0.44        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.61      0.62      0.61       133\n",
      "          20       0.52      0.46      0.49        70\n",
      "          21       0.65      0.81      0.72        27\n",
      "          22       0.33      0.14      0.20         7\n",
      "          23       0.64      0.58      0.61        12\n",
      "          24       0.61      0.58      0.59        19\n",
      "          25       0.91      0.68      0.78        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.50      0.25      0.33         4\n",
      "          28       0.67      0.20      0.31        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.71      0.42      0.53        12\n",
      "          31       0.89      0.62      0.73        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.67      0.80      0.73         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.27      0.27      0.27        11\n",
      "          37       0.33      0.50      0.40         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.75      0.60      0.67         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.60      1.00      0.75         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.68      0.59      0.61      2246\n",
      "weighted avg       0.78      0.78      0.78      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsvc_A = LinearSVC(C=1000, penalty='l1', max_iter=800, dual=False)\n",
    "lsvc_A.fit(tfidfv_A, y_train_A)\n",
    "\n",
    "print(\"A의 SVM accuracy:\", accuracy_score(y_test_A, lsvc_A.predict(tfidfv_A_test)))\n",
    "print(classification_report(y_test_A, lsvc_A.predict(tfidfv_A_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B의 SVM accuracy: 0.7689225289403384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.72      0.70      0.71       105\n",
      "           2       0.67      0.70      0.68        20\n",
      "           3       0.89      0.90      0.90       813\n",
      "           4       0.80      0.83      0.81       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       0.33      0.33      0.33         3\n",
      "           8       0.59      0.68      0.63        38\n",
      "           9       0.71      0.80      0.75        25\n",
      "          10       0.72      0.77      0.74        30\n",
      "          11       0.66      0.73      0.70        83\n",
      "          12       0.40      0.31      0.35        13\n",
      "          13       0.57      0.57      0.57        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.71      0.56      0.63         9\n",
      "          16       0.63      0.70      0.66        99\n",
      "          17       0.80      0.33      0.47        12\n",
      "          18       0.87      0.65      0.74        20\n",
      "          19       0.63      0.63      0.63       133\n",
      "          20       0.49      0.50      0.49        70\n",
      "          21       0.58      0.70      0.63        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.53      0.67      0.59        12\n",
      "          24       0.56      0.53      0.54        19\n",
      "          25       0.80      0.52      0.63        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.57      0.40      0.47        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.71      0.42      0.53        12\n",
      "          31       0.67      0.46      0.55        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.57      0.80      0.67         5\n",
      "          34       0.67      0.57      0.62         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.50      0.36      0.42        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.43      0.30      0.35        10\n",
      "          41       0.56      0.62      0.59         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.64      0.58      0.59      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsvc_B = LinearSVC(C=1000, penalty='l1', max_iter=800, dual=False)\n",
    "lsvc_B.fit(tfidfv_B, y_train_B)\n",
    "\n",
    "print(\"B의 SVM accuracy:\", accuracy_score(y_test_B, lsvc_B.predict(tfidfv_B_test)))\n",
    "print(classification_report(y_test_B, lsvc_B.predict(tfidfv_B_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C의 SVM accuracy: 0.7791629563668745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.72      0.70      0.71       105\n",
      "           2       0.72      0.65      0.68        20\n",
      "           3       0.91      0.91      0.91       813\n",
      "           4       0.80      0.86      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.60      0.66      0.62        38\n",
      "           9       0.84      0.84      0.84        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.59      0.72      0.65        83\n",
      "          12       0.36      0.38      0.37        13\n",
      "          13       0.61      0.54      0.57        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       0.62      0.72      0.67        99\n",
      "          17       0.80      0.33      0.47        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.63      0.65      0.64       133\n",
      "          20       0.52      0.46      0.48        70\n",
      "          21       0.59      0.74      0.66        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.69      0.75      0.72        12\n",
      "          24       0.64      0.47      0.55        19\n",
      "          25       0.95      0.61      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.38      0.75      0.50         4\n",
      "          30       0.50      0.25      0.33        12\n",
      "          31       0.80      0.62      0.70        13\n",
      "          32       0.83      1.00      0.91        10\n",
      "          33       0.67      0.80      0.73         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.29      0.20      0.24        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.66      0.59      0.60      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsvc_C = LinearSVC(C=1000, penalty='l1', max_iter=800, dual=False)\n",
    "lsvc_C.fit(tfidfv_C, y_train_C)\n",
    "\n",
    "print(\"C의 SVM accuracy:\", accuracy_score(y_test_C, lsvc_C.predict(tfidfv_C_test)))\n",
    "print(classification_report(y_test_C, lsvc_C.predict(tfidfv_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines의 accuracy\n",
    "\n",
    "A: 0.78\n",
    "B: 0.77\n",
    "C: 0.78\n",
    "    \n",
    "소수점 3자리에서 A가 가장 우세하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A의 Decision Tree accuracy: 0.6211041852181657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.69      0.43      0.53       105\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.94      0.85      0.89       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.16      0.28        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.58      0.60      0.59        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.83      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.67      0.41      0.50       133\n",
      "          20       0.83      0.07      0.13        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.60      0.19      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.22      0.15      0.15      2246\n",
      "weighted avg       0.62      0.62      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree_A = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree_A.fit(tfidfv_A, y_train_A)\n",
    "\n",
    "print(\"A의 Decision Tree accuracy:\", accuracy_score(y_test_A, tree_A.predict(tfidfv_A_test)))\n",
    "print(classification_report(y_test_A, tree_A.predict(tfidfv_A_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B의 Decision Tree accuracy: 0.6179875333926982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.40      0.52       105\n",
      "           2       0.60      0.45      0.51        20\n",
      "           3       0.94      0.84      0.89       813\n",
      "           4       0.39      0.91      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.57      0.73        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.48      0.54        83\n",
      "          12       0.17      0.08      0.11        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.82      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.26      0.37       133\n",
      "          20       0.33      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.24      0.17      0.18      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree_B = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree_B.fit(tfidfv_B, y_train_B)\n",
    "\n",
    "print(\"B의 Decision Tree accuracy:\", accuracy_score(y_test_B, tree_B.predict(tfidfv_B_test)))\n",
    "print(classification_report(y_test_B, tree_B.predict(tfidfv_B_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C의 Decision Tree accuracy: 0.6202137132680321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.42      0.53       105\n",
      "           2       0.62      0.50      0.56        20\n",
      "           3       0.93      0.83      0.88       813\n",
      "           4       0.40      0.90      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.90      0.64      0.75        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.64      0.51      0.56        83\n",
      "          12       0.14      0.08      0.10        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.84      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.29      0.39       133\n",
      "          20       0.27      0.06      0.09        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      1.00      1.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.23      0.18      0.18      2246\n",
      "weighted avg       0.61      0.62      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree_C = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree_C.fit(tfidfv_C, y_train_C)\n",
    "\n",
    "print(\"C의 Decision Tree accuracy:\", accuracy_score(y_test_C, tree_C.predict(tfidfv_C_test)))\n",
    "print(classification_report(y_test_C, tree_C.predict(tfidfv_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree의 accuracy\n",
    "\n",
    "A: 0.62\n",
    "B: 0.62\n",
    "C: 0.62\n",
    "\n",
    "소수점 3자리 이하에서 미세하게 B가 우세하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A의 Random Forest accuracy: 0.6544968833481746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.58      0.35        12\n",
      "           1       0.35      0.60      0.44       105\n",
      "           2       0.32      0.40      0.36        20\n",
      "           3       0.82      0.89      0.85       813\n",
      "           4       0.62      0.84      0.71       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.51      0.47      0.49        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.46      0.20      0.28        30\n",
      "          11       0.56      0.64      0.60        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.33      0.16      0.22        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.46      0.52        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.44      0.20      0.28        20\n",
      "          19       0.61      0.50      0.55       133\n",
      "          20       0.51      0.33      0.40        70\n",
      "          21       0.55      0.22      0.32        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.33      0.08      0.13        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       1.00      0.23      0.37        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.43      0.27      0.33        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.65      2246\n",
      "   macro avg       0.40      0.25      0.28      2246\n",
      "weighted avg       0.63      0.65      0.62      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest_A = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest_A.fit(tfidfv_A, y_train_A)\n",
    "\n",
    "print(\"A의 Random Forest accuracy:\", accuracy_score(y_test_A, forest_A.predict(tfidfv_A_test)))\n",
    "print(classification_report(y_test_A, forest_A.predict(tfidfv_A_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B의 Random Forest accuracy: 0.701246660730187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.33        12\n",
      "           1       0.42      0.78      0.55       105\n",
      "           2       0.44      0.35      0.39        20\n",
      "           3       0.84      0.90      0.87       813\n",
      "           4       0.68      0.84      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.43      0.57        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.53      0.56        38\n",
      "           9       0.71      0.40      0.51        25\n",
      "          10       0.89      0.53      0.67        30\n",
      "          11       0.57      0.69      0.62        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.46      0.32      0.38        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       1.00      0.11      0.20         9\n",
      "          16       0.70      0.67      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.60      0.45      0.51        20\n",
      "          19       0.62      0.64      0.63       133\n",
      "          20       0.46      0.33      0.38        70\n",
      "          21       0.65      0.41      0.50        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.25      0.38        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.87      0.42      0.57        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.25      0.12      0.17         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.54      0.31      0.36      2246\n",
      "weighted avg       0.69      0.70      0.68      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest_B = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest_B.fit(tfidfv_B, y_train_B)\n",
    "\n",
    "print(\"B의 Random Forest accuracy:\", accuracy_score(y_test_B, forest_B.predict(tfidfv_B_test)))\n",
    "print(classification_report(y_test_B, forest_B.predict(tfidfv_B_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C의 Random Forest accuracy: 0.674087266251113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.33      0.27        12\n",
      "           1       0.45      0.77      0.57       105\n",
      "           2       0.30      0.30      0.30        20\n",
      "           3       0.82      0.90      0.86       813\n",
      "           4       0.61      0.83      0.70       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.67      0.53      0.59        38\n",
      "           9       0.70      0.28      0.40        25\n",
      "          10       0.75      0.30      0.43        30\n",
      "          11       0.55      0.59      0.57        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.37      0.19      0.25        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.59      0.59        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.50      0.25      0.33        20\n",
      "          19       0.69      0.54      0.61       133\n",
      "          20       0.57      0.29      0.38        70\n",
      "          21       0.67      0.30      0.41        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       1.00      0.32      0.49        31\n",
      "          26       1.00      0.25      0.40         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.50      0.08      0.14        12\n",
      "          31       0.67      0.15      0.25        13\n",
      "          32       0.67      0.20      0.31        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.50      0.14      0.22         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.67      0.33      0.44         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.46      0.27      0.31      2246\n",
      "weighted avg       0.66      0.67      0.64      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest_C = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest_C.fit(tfidfv_C, y_train_C)\n",
    "\n",
    "print(\"C의 Random Forest accuracy:\", accuracy_score(y_test_C, forest_C.predict(tfidfv_C_test)))\n",
    "print(classification_report(y_test_C, forest_C.predict(tfidfv_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest accuracy\n",
    "\n",
    "A: 0.65\n",
    "B: 0.70\n",
    "C: 0.67\n",
    "\n",
    "B의 성능이 가장 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A의 GRBT accuracy: 0.7702582368655387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55        12\n",
      "           1       0.81      0.71      0.76       105\n",
      "           2       0.58      0.70      0.64        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.78      0.86      0.82       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.77      0.71      0.74        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.60      0.63      0.62        38\n",
      "           9       0.91      0.80      0.85        25\n",
      "          10       0.79      0.77      0.78        30\n",
      "          11       0.61      0.65      0.63        83\n",
      "          12       0.50      0.46      0.48        13\n",
      "          13       0.48      0.32      0.39        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.25      0.11      0.15         9\n",
      "          16       0.72      0.71      0.71        99\n",
      "          17       0.83      0.42      0.56        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.71      0.64      0.67       133\n",
      "          20       0.64      0.41      0.50        70\n",
      "          21       0.61      0.63      0.62        27\n",
      "          22       0.33      0.14      0.20         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.83      0.65      0.73        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.33      0.50      0.40         4\n",
      "          28       0.25      0.20      0.22        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.36      0.42      0.38        12\n",
      "          31       0.50      0.54      0.52        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.60      0.43      0.50         7\n",
      "          35       0.33      0.17      0.22         6\n",
      "          36       0.50      0.64      0.56        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.83      0.50      0.62        10\n",
      "          41       0.62      0.62      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.43      0.50      0.46         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.62      0.57      0.57      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grbt_A = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt_A.fit(tfidfv_A, y_train_A)\n",
    "\n",
    "print(\"A의 GRBT accuracy:\", accuracy_score(y_test_A, grbt_A.predict(tfidfv_A_test)))\n",
    "print(classification_report(y_test_A,  grbt_A.predict(tfidfv_A_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B의 GRBT accuracy: 0.767586821015138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.80      0.68      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.14      0.20      0.17         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.66      0.64        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.55      0.43      0.48        37\n",
      "          14       0.08      0.50      0.14         2\n",
      "          15       0.33      0.22      0.27         9\n",
      "          16       0.72      0.77      0.75        99\n",
      "          17       0.33      0.33      0.33        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.71      0.65      0.68       133\n",
      "          20       0.56      0.44      0.50        70\n",
      "          21       0.67      0.67      0.67        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.36      0.42      0.38        12\n",
      "          24       0.71      0.63      0.67        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.40      0.50      0.44         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.22      0.50      0.31         4\n",
      "          30       0.38      0.42      0.40        12\n",
      "          31       0.60      0.46      0.52        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.25      0.33      0.29         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.71      0.50      0.59        10\n",
      "          41       0.44      0.50      0.47         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.60      0.59      0.58      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grbt_B = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt_B.fit(tfidfv_B, y_train_B)\n",
    "\n",
    "print(\"B의 GRBT accuracy:\", accuracy_score(y_test_B, grbt_B.predict(tfidfv_B_test)))\n",
    "print(classification_report(y_test_B,  grbt_B.predict(tfidfv_B_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C의 GRBT accuracy: 0.7666963490650045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.77      0.68      0.72       105\n",
      "           2       0.78      0.70      0.74        20\n",
      "           3       0.88      0.91      0.89       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.80      0.86      0.83        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.74      0.80      0.77        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.63      0.64      0.63        83\n",
      "          12       0.33      0.46      0.39        13\n",
      "          13       0.62      0.49      0.55        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.38      0.33      0.35         9\n",
      "          16       0.73      0.73      0.73        99\n",
      "          17       0.27      0.25      0.26        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.69      0.65      0.67       133\n",
      "          20       0.67      0.46      0.54        70\n",
      "          21       0.70      0.78      0.74        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.54      0.58      0.56        12\n",
      "          24       0.61      0.58      0.59        19\n",
      "          25       0.89      0.55      0.68        31\n",
      "          26       0.71      0.62      0.67         8\n",
      "          27       0.50      0.50      0.50         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.23      0.75      0.35         4\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.62      0.38      0.48        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.75      0.60      0.67         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       0.80      0.67      0.73         6\n",
      "          36       0.62      0.45      0.53        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.40      0.20      0.27        10\n",
      "          41       0.56      0.62      0.59         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.67      0.67      0.67         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.58      0.58      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grbt_C = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt_C.fit(tfidfv_C, y_train_C)\n",
    "\n",
    "print(\"C의 GRBT accuracy:\", accuracy_score(y_test_C, grbt_C.predict(tfidfv_C_test)))\n",
    "print(classification_report(y_test_C,  grbt_C.predict(tfidfv_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Tree의 accuracy\n",
    "\n",
    "A: 0.77\n",
    "B: 0.77\n",
    "C: 0.77\n",
    "\n",
    "소수점 3자리에서 A의 성능이 우세하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=10000)),\n",
       "                             ('cb', ComplementNB()),\n",
       "                             ('grbt',\n",
       "                              GradientBoostingClassifier(random_state=0))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_classifier_A = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier_A.fit(tfidfv_A, y_train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A의 soft Voting: 0.8187889581478184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.93      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.67      0.71      0.69        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.69      0.65      0.67        37\n",
      "          14       0.29      1.00      0.44         2\n",
      "          15       0.40      0.22      0.29         9\n",
      "          16       0.73      0.76      0.74        99\n",
      "          17       0.75      0.50      0.60        12\n",
      "          18       0.73      0.55      0.63        20\n",
      "          19       0.71      0.71      0.71       133\n",
      "          20       0.66      0.50      0.57        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.73      0.58      0.65        19\n",
      "          25       0.92      0.77      0.84        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.50      1.00      0.67         4\n",
      "          30       0.54      0.58      0.56        12\n",
      "          31       0.82      0.69      0.75        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.54      0.64      0.58        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       1.00      0.40      0.57        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.83      0.83      0.83         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.73      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"A의 soft Voting:\", accuracy_score(y_test_A, voting_classifier_A.predict(tfidfv_A_test))) #예측값과 실제값 비교\n",
    "print(classification_report(y_test_A, voting_classifier_A.predict(tfidfv_A_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=10000)),\n",
       "                             ('cb', ComplementNB()),\n",
       "                             ('grbt',\n",
       "                              GradientBoostingClassifier(random_state=0))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_classifier_B = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier_B.fit(tfidfv_B, y_train_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B의 soft Voting: 0.8161175422974176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.71      0.85      0.77        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.72      0.68      0.70        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.67      0.70      0.68        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.12      0.50      0.20         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.74      0.74      0.74        99\n",
      "          17       0.57      0.67      0.62        12\n",
      "          18       0.72      0.65      0.68        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.61      0.49      0.54        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.75      0.63      0.69        19\n",
      "          25       0.96      0.74      0.84        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.75      0.69      0.72        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.80      0.40      0.53        10\n",
      "          41       0.67      0.50      0.57         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"B의 soft Voting:\", accuracy_score(y_test_B, voting_classifier_B.predict(tfidfv_B_test))) #예측값과 실제값 비교\n",
    "print(classification_report(y_test_B, voting_classifier_B.predict(tfidfv_B_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=10000)),\n",
       "                             ('cb', ComplementNB()),\n",
       "                             ('grbt',\n",
       "                              GradientBoostingClassifier(random_state=0))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_classifier_C = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier_C.fit(tfidfv_C, y_train_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C의 soft Voting: 0.8116651825467498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.77      0.74      0.76       105\n",
      "           2       0.73      0.80      0.76        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.83      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.70      0.68      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.65      0.69      0.67        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.57      0.44      0.50         9\n",
      "          16       0.72      0.75      0.73        99\n",
      "          17       0.53      0.67      0.59        12\n",
      "          18       0.79      0.55      0.65        20\n",
      "          19       0.68      0.69      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.65      0.81      0.72        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.60      0.75      0.67        12\n",
      "          24       0.67      0.63      0.65        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.40      1.00      0.57         4\n",
      "          30       0.67      0.50      0.57        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"C의 soft Voting:\", accuracy_score(y_test_C, voting_classifier_C.predict(tfidfv_C_test))) #예측값과 실제값 비교\n",
    "print(classification_report(y_test_C, voting_classifier_C.predict(tfidfv_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting의 accuracy\n",
    "\n",
    "A: 0.82\n",
    "B: 0.82\n",
    "C: 0.81\n",
    "\n",
    "소수점 2째자리에서 A우세"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 모델과 비교해 보기\n",
    "\n",
    "SVM을 제외하고 다른 머신러닝 모델에서 B의 성능이 가장 좋거나 유의미한 차이가 없었기 때문에 해당 과정에선 word_num을 5000으로 제한한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words = 5000, test_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 8982, 테스트 개수: 2246\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "word_to_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  145.96419665122906\n",
      "문장길이 최대 :  2376\n",
      "문장길이 표준편차 :  145.8784764459447\n",
      "pad_sequences maxlen :  437\n",
      "전체 문장의 0.9438902743142145%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 437)\n",
      "(2246, 437)\n"
     ]
    }
   ],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796.4\n",
      "359.4\n"
     ]
    }
   ],
   "source": [
    "print(8982/5)\n",
    "print(1797/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=46, dtype='float32')\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=46, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7186, 437)\n",
      "(7186, 46)\n"
     ]
    }
   ],
   "source": [
    "# validation set 분리\n",
    "x_val = x_train[:1796]   \n",
    "y_val = y_train[:1796]\n",
    "\n",
    "# validation set을 제외한 나머지\n",
    "partial_x_train = x_train[1796:]  \n",
    "partial_y_train = y_train[1796:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 92)          460000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 92)                68080     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                4278      \n",
      "=================================================================\n",
      "Total params: 532,358\n",
      "Trainable params: 532,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000 # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 92  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim))\n",
    "model.add(keras.layers.LSTM(92))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(46, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.2748 - accuracy: 0.0135 - val_loss: 0.2791 - val_accuracy: 0.0156\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.2339 - accuracy: 0.0324 - val_loss: 0.1983 - val_accuracy: 0.2227\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.1993 - accuracy: 0.2192 - val_loss: 0.1999 - val_accuracy: 0.2238\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.1991 - accuracy: 0.2181 - val_loss: 0.1987 - val_accuracy: 0.2233\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.1313 - accuracy: 0.2545 - val_loss: 0.1001 - val_accuracy: 0.3419\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.0957 - accuracy: 0.3542 - val_loss: 0.0982 - val_accuracy: 0.3419\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.0949 - accuracy: 0.3542 - val_loss: 0.0996 - val_accuracy: 0.3419\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.0973 - accuracy: 0.3542 - val_loss: 0.1070 - val_accuracy: 0.3419\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.1037 - accuracy: 0.3542 - val_loss: 0.1091 - val_accuracy: 0.3419\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.1024 - accuracy: 0.3543 - val_loss: 0.1054 - val_accuracy: 0.3419\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 안되는 문제가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 - 0s - loss: 0.1028 - accuracy: 0.3620\n",
      "[0.10283476114273071, 0.36197686195373535]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyl0lEQVR4nO3deZhU1bX38e+PeRRkUJGpMUGRyNDS4IDzcINiGDpiRAyitERvRr0xYkiirwl5k+ibeL3XeEWN022DxgGJQzSKiHMYRBDEiArYiBFRJgGZ1vvHPg1FU91d1d2nT3X3+jzPearOPkOtqoZatfc+Z2+ZGc4551ymGiUdgHPOubrFE4dzzrmseOJwzjmXFU8czjnnsuKJwznnXFY8cTjnnMuKJw6XOElPSbqopvdNkqQVks6I4bwm6avR8/+R9PNM9q3C64yT9ExV46zgvKdIKqnp87ra1STpAFzdJGlzymor4EtgV7T+HTMrzvRcZnZWHPvWd2Z2WU2cR1Ie8AHQ1Mx2RucuBjL+G7qGxROHqxIza1P6XNIKoMjMni27n6QmpV9Gzrn6wZuqXI0qbYqQdLWkj4G7JB0o6XFJayV9Hj3vlnLMbElF0fMJkl6SdGO07weSzqrivr0kzZG0SdKzkm6R9L/lxJ1JjL+U9HJ0vmckdUrZ/m1JKyWtkzSlgs/nGEkfS2qcUjZa0qLo+RBJr0paL2mNpP+W1Kycc90t6Vcp61dFx3wk6ZIy+w6X9IakjZI+lHRdyuY50eN6SZslHVf62aYcf7ykuZI2RI/HZ/rZVETSkdHx6yUtkTQiZdvZkpZG51wt6cdReafo77Ne0meSXpTk32W1yD9sF4dDgA5AT2AS4d/ZXdF6D2Ar8N8VHH8M8A7QCfgdcKckVWHf+4F/AB2B64BvV/CamcR4AXAxcBDQDCj9IusL3Bqd/9Do9bqRhpm9DnwBnFbmvPdHz3cBV0Tv5zjgdODfK4ibKIZhUTxnAr2Bsv0rXwDjgfbAcOBySaOibSdFj+3NrI2ZvVrm3B2AJ4Cbo/f2e+AJSR3LvIf9PptKYm4K/BV4Jjru+0CxpCOiXe4kNHu2BY4CZkXl/wGUAJ2Bg4GfAj52Ui3yxOHisBu41sy+NLOtZrbOzB42sy1mtgmYCpxcwfErzex2M9sF3AN0IXxBZLyvpB7AYOAXZrbdzF4CZpb3ghnGeJeZ/dPMtgIPAgOj8nOBx81sjpl9Cfw8+gzK82dgLICktsDZURlmNt/MXjOznWa2ArgtTRzpnBfF95aZfUFIlKnvb7aZLTaz3Wa2KHq9TM4LIdG8a2b3RXH9GVgGfCNln/I+m4ocC7QBfhP9jWYBjxN9NsAOoK+kA8zsczNbkFLeBehpZjvM7EXzQfdqlScOF4e1ZratdEVSK0m3RU05GwlNI+1Tm2vK+Lj0iZltiZ62yXLfQ4HPUsoAPiwv4Axj/Djl+ZaUmA5NPXf0xb2uvNci1C4KJTUHCoEFZrYyiuPwqBnm4yiOXxNqH5XZJwZgZZn3d4yk56OmuA3AZRmet/TcK8uUrQS6pqyX99lUGrOZpSbZ1PN+k5BUV0p6QdJxUfkNwHLgGUnvS5qc2dtwNcUTh4tD2V9//wEcARxjZgewt2mkvOanmrAG6CCpVUpZ9wr2r06Ma1LPHb1mx/J2NrOlhC/Is9i3mQpCk9cyoHcUx0+rEgOhuS3V/YQaV3czawf8T8p5K/u1/hGhCS9VD2B1BnFVdt7uZfon9pzXzOaa2UhCM9YMQk0GM9tkZv9hZocBI4ArJZ1ezVhcFjxxuNrQltBnsD5qL7827heMfsHPA66T1Cz6tfqNCg6pTowPAedIOiHqyL6eyv9v3Q/8kJCg/lImjo3AZkl9gMszjOFBYIKkvlHiKht/W0INbJukIYSEVWotoWntsHLO/SRwuKQLJDWR9C2gL6FZqTpeJ9ROfiKpqaRTCH+j6dHfbJykdma2g/CZ7AaQdI6kr0Z9WRsI/UIVNQ26GuaJw9WGm4CWwKfAa8Dfaul1xxE6mNcBvwIeINxvks5NVDFGM1sCfJeQDNYAnxM6bytS2scwy8w+TSn/MeFLfRNwexRzJjE8Fb2HWYRmnFlldvl34HpJm4BfEP16j47dQujTeTm6UunYMudeB5xDqJWtA34CnFMm7qyZ2XZCojiL8Ln/ERhvZsuiXb4NrIia7C4j/D0hdP4/C2wGXgX+aGbPVycWlx15n5JrKCQ9ACwzs9hrPM7VZ17jcPWWpMGSviKpUXS56khCW7lzrhr8znFXnx0CPELoqC4BLjezN5INybm6z5uqnHPOZcWbqpxzzmWlQTRVderUyfLy8pIOwznn6pT58+d/amady5Y3iMSRl5fHvHnzkg7DOefqFEllRwwAvKnKOedcljxxOOecy4onDuecc1lpEH0czrnat2PHDkpKSti2bVvlO7tEtWjRgm7dutG0adOM9vfE4ZyLRUlJCW3btiUvL4/y5+FySTMz1q1bR0lJCb169croGG+qcs7FYtu2bXTs2NGTRo6TRMeOHbOqGXricM7FxpNG3ZDt38kTRwUefBBuuy3pKJxzLrd44qjAQw/BlCnwZXkzODjnctK6desYOHAgAwcO5JBDDqFr16571rdv317hsfPmzeMHP/hBpa9x/PHH10iss2fP5pxzzqmRc9WWWBOHpGGS3pG0PN28wJKulLRU0iJJz0nqGZWfKmlhyrJN0qho292SPkjZNjCu+IuKYN06eOyxuF7BOVequBjy8qBRo/BYXFz1c3Xs2JGFCxeycOFCLrvsMq644oo9682aNWPnzp3lHltQUMDNN99c6Wu88sorVQ+wjostcUhqDNxCmN2rLzBWUt8yu70BFJhZf8L0m78DMLPnzWygmQ0ETiNML/lMynFXlW43s4VxxF9cDJdeGp5fdFH1/hE75ypWXAyTJsHKlWAWHidNqtn/dxMmTOCyyy7jmGOO4Sc/+Qn/+Mc/OO6448jPz+f444/nnXfeAfatAVx33XVccsklnHLKKRx22GH7JJQ2bdrs2f+UU07h3HPPpU+fPowbN47SUceffPJJ+vTpw6BBg/jBD35Qac3is88+Y9SoUfTv359jjz2WRYsWAfDCCy/sqTHl5+ezadMm1qxZw0knncTAgQM56qijePHFF2vuw6pEnJfjDgGWm9n7AJKmEybSWVq6Q5npHl8DLkxznnOBp6LpLWtF6T/iLdErbtsWah8A48aVf5xzrmqmTNn7/63Uli2hvCb/z5WUlPDKK6/QuHFjNm7cyIsvvkiTJk149tln+elPf8rDDz+83zHLli3j+eefZ9OmTRxxxBFcfvnl+93v8MYbb7BkyRIOPfRQhg4dyssvv0xBQQHf+c53mDNnDr169WLs2LGVxnfttdeSn5/PjBkzmDVrFuPHj2fhwoXceOON3HLLLQwdOpTNmzfTokULpk2bxte//nWmTJnCrl272FL2A4xRnE1VXYEPU9ZLorLyTASeSlN+PmF+5lRTo+atP0hqXr0w95fuH/G2baHcOVfzVq3KrryqxowZQ+PGjQHYsGEDY8aM4aijjuKKK65gyZIlaY8ZPnw4zZs3p1OnThx00EH861//2m+fIUOG0K1bNxo1asTAgQNZsWIFy5Yt47DDDttzb0QmieOll17i29/+NgCnnXYa69atY+PGjQwdOpQrr7ySm2++mfXr19OkSRMGDx7MXXfdxXXXXcfixYtp27ZtVT+WrOVE57ikC4EC4IYy5V2AfsDTKcXXAH2AwUAH4OpyzjlJ0jxJ89auXZtVPOX9Y12ZdpxI51x19eiRXXlVtW7des/zn//855x66qm89dZb/PWvfy33Pobmzff+Nm3cuHHa/pFM9qmOyZMnc8cdd7B161aGDh3KsmXLOOmkk5gzZw5du3ZlwoQJ3HvvvTX6mhWJM3GsBrqnrHeLyvYh6QxgCjDCzMpev3Qe8KiZ7SgtMLM1FnwJ3EVoEtuPmU0zswIzK+jceb/h5CtU3j/Wgw7K6jTOuQxNnQqtWu1b1qpVKI/Lhg0b6No1NILcfffdNX7+I444gvfff58VK1YA8MADD1R6zIknnkhx1LEze/ZsOnXqxAEHHMB7771Hv379uPrqqxk8eDDLli1j5cqVHHzwwVx66aUUFRWxYMGCGn8P5YkzccwFekvqJakZoclpZuoOkvKB2whJ45M05xhLmWaqqBaCwh0ro4C3ajrwdP+IAXr2rOlXcs5B6MeYNi38H5PC47Rp8fYp/uQnP+Gaa64hPz+/xmsIAC1btuSPf/wjw4YNY9CgQbRt25Z27dpVeMx1113H/Pnz6d+/P5MnT+aee+4B4KabbuKoo46if//+NG3alLPOOovZs2czYMAA8vPzeeCBB/jhD39Y4++hPLHOOS7pbOAmoDHwJzObKul6YJ6ZzZT0LKEpak10yCozGxEdmwe8DHQ3s90p55wFdAYELAQuM7PNFcVRUFBg2U7kVFwc+jRWrQo1kK99DZ55BkpK4OCDszqVcw3S22+/zZFHHpl0GInavHkzbdq0wcz47ne/S+/evbniiiuSDiutdH8vSfPNrKDsvrEOcmhmTwJPlin7RcrzMyo4dgVpOtPN7LQaDLFc48bt+2tn2TI48ki491646qraiMA5V9fdfvvt3HPPPWzfvp38/Hy+853vJB1SjYi1xpErqlLjSOeEE2Dt2pBEfAge5yrmNY66JZsaR05cVVVXFBXBP/8JL7+cdCTOOZccTxxZGDMG2raFO+5IOhLnnEuOJ44stG4NF1wQRs3dsCHpaJxzLhmeOLJUVARbt8Kfy97L7pxzDYQnjiwNGgQDBnhzlXO57NRTT+Xpp5/ep+ymm27i8ssvL/eYU045hdKLaM4++2zWr1+/3z7XXXcdN954Y4WvPWPGDJYu3TMkH7/4xS949tlns4g+vVwaft0TR5YkmDgR5s+HN95IOhrnXDpjx45l+vTp+5RNnz49o/GiIIxq2759+yq9dtnEcf3113PGGeXeeVAneeKognHjoHlzuPPOpCNxzqVz7rnn8sQTT+yZtGnFihV89NFHnHjiiVx++eUUFBTwta99jWuvvTbt8Xl5eXz66acATJ06lcMPP5wTTjhhz9DrEO7RGDx4MAMGDOCb3/wmW7Zs4ZVXXmHmzJlcddVVDBw4kPfee48JEybw0EMPAfDcc8+Rn59Pv379uOSSS/gymiUuLy+Pa6+9lqOPPpp+/fqxbNmyCt9f0sOvx3oDYH3VoQN885vh7vIbboCWLZOOyLnc9qMfwcKFNXvOgQPhppvSb+vQoQNDhgzhqaeeYuTIkUyfPp3zzjsPSUydOpUOHTqwa9cuTj/9dBYtWkT//v3Tnmf+/PlMnz6dhQsXsnPnTo4++mgGDRoEQGFhIZdGk/b87Gc/48477+T73/8+I0aM4JxzzuHcc8/d51zbtm1jwoQJPPfccxx++OGMHz+eW2+9lR/96EcAdOrUiQULFvDHP/6RG2+8kTsqaA9Pevh1r3FUUVERrF8PjzySdCTOuXRSm6tSm6kefPBBjj76aPLz81myZMk+zUplvfjii4wePZpWrVpxwAEHMGLEiD3b3nrrLU488UT69etHcXFxucOyl3rnnXfo1asXhx9+OAAXXXQRc+bM2bO9sLAQgEGDBu0ZGLE8SQ+/7jWOKjr5ZPjKV0InuU/u5FzFyqsZxGnkyJFcccUVLFiwgC1btjBo0CA++OADbrzxRubOncuBBx7IhAkTyh1OvTITJkxgxowZDBgwgLvvvpvZs2dXK97SodmrMyz75MmTGT58OE8++SRDhw7l6aef3jP8+hNPPMGECRO48sorGT9+fLVi9RpHFTVqFDrJZ8+Gd99NOhrnXFlt2rTh1FNP5ZJLLtlT29i4cSOtW7emXbt2/Otf/+Kpp9LNHbfXSSedxIwZM9i6dSubNm3ir3/9655tmzZtokuXLuzYsWPPUOgAbdu2ZdOmTfud64gjjmDFihUsX74cgPvuu4+TTz65Su8t6eHXPXFUw0UXQePG8Kc/JR2Jcy6dsWPH8uabb+5JHKXDkPfp04cLLriAoUOHVnj80Ucfzbe+9S0GDBjAWWedxeDBg/ds++Uvf8kxxxzD0KFD6dOnz57y888/nxtuuIH8/Hzee++9PeUtWrTgrrvuYsyYMfTr149GjRpx2WWXVel9JT38ug9yWE0jRsDcuWH49TLTEDvXoPkgh3WLD3JYi4qK4OOP4cknK9/XOefqA08c1XT22dCli9/T4ZxrODxxVFOTJjBhAjzxBKzeb0Z15xq2htAUXh9k+3fyxFEDLrkEdu+GqH/KOUfoDF63bp0njxxnZqxbt44WLVpkfEzcc44PA/6TMOf4HWb2mzLbrwSKgJ3AWuASM1sZbdsFLI52TZ2LvBcwHegIzAe+bWbbK4ojzs7xUqedBitXhktzG3k6do4dO3ZQUlJS5fskXO1p0aIF3bp1o2mZK3xqfc5xSY2BW4AzgRJgrqSZZpZ6m+YbQIGZbZF0OfA74FvRtq1mNjDNqX8L/MHMpkv6H2AicGtc7yNTEyfChReG+zpOq5VZ0Z3LbU2bNqVXr15Jh+FiEOdv4yHAcjN7P6oRTAdGpu5gZs+bWenAKa8B3So6oSQBpwEPRUX3AKNqMuiqKiyE9u19uHXnXP0XZ+LoCnyYsl4SlZVnIpB6G2cLSfMkvSZpVFTWEVhvZqX345d7TkmTouPnrV27tkpvIBstW4YaxyOPwGefxf5yzjmXmJxojZd0IVAA3JBS3DNqW7sAuEnSV7I5p5lNM7MCMyvo3LlzDUZbvqIi+PLLMGquc87VV3EmjtVA95T1blHZPiSdAUwBRpjZl6XlZrY6enwfmA3kA+uA9pJK+2bSnjMpAwZAQQHcfjv4hSTOufoqzsQxF+gtqZekZsD5wMzUHSTlA7cRksYnKeUHSmoePe8EDAWWWrgE7HmgdKD7i4DHYnwPWSsqgsWLIeaLuJxzLjGxJY6oH+J7wNPA28CDZrZE0vWSSge1vwFoA/xF0kJJpYnlSGCepDcJieI3KVdjXQ1cKWk5oc8jp+7ZHjsWWrXyTnLnXP3lgxzGYMIEePhhWLMG2rSptZd1zrka5YMc1qKiIti8Gf7yl6Qjcc65mueJIwZDh8IRR/jAh865+skTRwykUOt4+WV4++2ko3HOuZrliSMm48eHkXO91uGcq288ccTkoINg5MgwYu72CodgdM65usUTR4yKiuDTT2HmzMr3dc65usITR4zOPBO6d/d7Opxz9Ysnjhg1bgwXXwzPPBPm6nDOufrAE0fMLr44PN59d6JhOOdcjfHEEbO8vNBk9ac/wa5d2R9fXBzO0ahRePSRd51zSfPEUQuKimDVKnj22eyOKy6GSZNCM5dZeJw0yZOHcy5ZnjhqwYgR0KlT9p3kU6bAli37lm3ZEsqdcy4pnjhqQfPm4YbAxx6DTz6pfP9Sq1ZlV+6cc7XBE0ctmTgRduyA++7L/JgePbIrd8652uCJo5b07QvHHReaqzIdyX7q1DC3R6pWrUK5c84lxRNHLSoqgmXL4NVXM9t/3DiYNg169gwDJ/bsGdbHjYs3Tuecq4hP5FSLNm+GLl1gzJhwea5zzuUyn8gpB7RpE6aWfeAB2Lgx6Wicc65qYk0ckoZJekfSckmT02y/UtJSSYskPSepZ1Q+UNKrkpZE276Vcszdkj6I5ihfKGlgnO+hphUVhUtqp09POhLnnKua2BKHpMbALcBZQF9grKS+ZXZ7Aygws/7AQ8DvovItwHgz+xowDLhJUvuU464ys4HRsjCu9xCHwYOhXz8f+NA5V3fFWeMYAiw3s/fNbDswHRiZuoOZPW9mpbe4vQZ0i8r/aWbvRs8/Aj4BOscYa62RwqW5c+fCokVJR+Occ9mLM3F0BT5MWS+JysozEXiqbKGkIUAz4L2U4qlRE9YfJDVPdzJJkyTNkzRv7dq12UcfowsvhGbNfHZA51zdlBOd45IuBAqAG8qUdwHuAy42s91R8TVAH2Aw0AG4Ot05zWyamRWYWUHnzrlVWenYEQoLw82A27YlHY1zzmUnzsSxGuiest4tKtuHpDOAKcAIM/sypfwA4Algipm9VlpuZmss+BK4i9AkVucUFcHnn8OjjyYdiXPOZSfOxDEX6C2pl6RmwPnAPpOoSsoHbiMkjU9SypsBjwL3mtlDZY7pEj0KGAW8FeN7iM2pp0KvXt5J7pyre2JLHGa2E/ge8DTwNvCgmS2RdL2kEdFuNwBtgL9El9aWJpbzgJOACWkuuy2WtBhYDHQCfhXXe4hTo0ahk3zWLHjvvcr3d865XOF3jido9eowYOHkyT7+lHMu9/id4zmoa1c46yy46y7YuTPpaJxzLjOeOBJWVARr1sDf/pZ0JM45lxlPHAkbPhwOPtg7yZ1zdYcnjoQ1bQoTJsDjj4eah3PO5TpPHDlg4kTYtQvuuSfpSJxzrnKeOHJA795w8snZzQ7onHNJ8cSRI4qKwv0cL7yQdCTOOVcxTxw54pvfhHbtvJPcOZf7PHHkiJYtw1ziDz8cxrByzrlc5YkjhxQVhdFy778/6Uicc658njhySH4+HH003H67d5I753KXJ44cU1QEb74JCxYkHYlzzqXniSPHjB0b+ju8k9w5l6s8ceSY9u1hzJjQz/HFF0lH45xz+/PEkYMmToSNG+Ghhyrf1znnapsnjhx04onhbvI770w6Euec258njhwkhU7yF1+Ed95JOhrnnNuXJ44cNX48NGnitQ7nXO6JNXFIGibpHUnLJU1Os/1KSUslLZL0nKSeKdsukvRutFyUUj5I0uLonDdLUpzvISmHHALf+EYYMXf79qSjcc65vWJLHJIaA7cAZwF9gbGS+pbZ7Q2gwMz6Aw8Bv4uO7QBcCxwDDAGulXRgdMytwKVA72gZFtd7SFpREXzySZirI2nFxZCXB40ahcfi4oYZg3MOmsR47iHAcjN7H0DSdGAksLR0BzN7PmX/14ALo+dfB/5uZp9Fx/4dGCZpNnCAmb0Wld8LjAKeivF9JObrXw/zkt9xBxQWxvc6ZrBlC2zatHfZvHnv82efhXvvhR07wv4rV8LFF4c+mDPOCPedlC6tWu2/3qJF+LKvjuJimDQpxFkaw6RJ4fm4cdU7t3MuO3Emjq7AhynrJYQaRHkmsjcBpDu2a7SUpCnfj6RJwCSAHj16ZBN3zmjcOHxB//rX8OGH0L17KDeDrVvL/6Ivu1S2bfNm2L07u9h27IDbbgtLJpo3Lz+xZLJ+3XV7k0apLVtgyhRPHM7VtjgTR8YkXQgUACfX1DnNbBowDaCgoKDOjvx0ySXwq1/BMceEX+2lSSDTL/oWLaBt232XTp2gV6/9y0uXNm32Xe9btoExxeLF4Qt869awpD7PdH3duvTbv/yy8ve3alVmn4NzrubEmThWA91T1rtFZfuQdAYwBTjZzL5MOfaUMsfOjsq7VXbO+qRXL/jlL8P4VeV9sZf3xd+mTZjTvLp69gxNQ+nKjzqq+ucvz65dYbTgrVth4EBYneYvXUcrk87VaXEmjrlAb0m9CF/u5wMXpO4gKR+4DRhmZp+kbHoa+HVKh/i/AdeY2WeSNko6FngdGA/8V4zvISf87GfJvv7Uqfv2L0BoQpo6Nd7XbdwYWrcOy29/u38MLVvGH4Nzbn+xXVVlZjuB7xGSwNvAg2a2RNL1kkZEu90AtAH+ImmhpJnRsZ8BvyQkn7nA9aUd5cC/A3cAy4H3qKcd47lk3DiYNi3UMKTwOG1a7fYtpMZQ6oorvH/DuSTIGsDEDwUFBTZv3rykw3A15IsvQj/NpZfCzTcnHY1z9Zek+WZWULY8oxqHpNaSGkXPD5c0QlINtJ47l73WrWHYMHjkkeyvBnPOVV+mTVVzgBaSugLPAN8G7o4rKOcqU1gYOsu9Iulc7cs0ccjMtgCFwB/NbAzwtfjCcq5i55wTxvJ69NGkI3Gu4ck4cUg6DhgHPBGVNY4nJOcqd+CBcOqp8PDDPj+7c7Ut08TxI+Aa4NHoyqjDgOcrPsS5eBUWwrvvwtKlle/rnKs5GSUOM3vBzEaY2W+jTvJPzewHMcfmXIVGjgyXBz/ySNKRONewZHpV1f2SDpDUGngLWCrpqnhDc65iXbrA8cd74nCutmXaVNXXzDaydyTaXoQrq5xLVGEhLFwIH3yQdCTONRyZJo6m0X0bo4CZZrYD8C5Jl7jRo8OjX13lXO3JNHHcBqwAWgNzopn6NsYVlHOZ6tUrDIDozVXO1Z5MO8dvNrOuZna2BSuBU2OOzbmMFBbCK6/AmjVJR+Jcw5Bp53g7Sb+XNC9a/h+h9uFc4goLw70cjz2WdCTONQyZNlX9CdgEnBctG4G74grKuWz07QuHH+7NVc7VlkwTx1fM7Fozez9a/g9wWJyBOZcpKdQ6nn8ePv886Wicq/8yTRxbJZ1QuiJpKLA1npCcy15hIezcCY8/nnQkztV/mSaOy4BbJK2QtAL4b+A7sUXlXJYKCqBbN2+ucq42ZHpV1ZtmNgDoD/Q3s3zgtFgjcy4LUrin429/CxM9Oefik9XUsWa2MbqDHODKGOJxrsoKC2HbtpA8nHPxqc6c46p0B2mYpHckLZc0Oc32kyQtkLRT0rkp5adGc5CXLtskjYq23S3pg5RtA6vxHlw9csIJYUpZb65yLl5NqnFshUOOSGoM3AKcCZQAcyXNNLPUQbBXAROAH+9zYrPngYHReToAywkzD5a6ysweqkbsrh5q0iSMmPuXv8D27dCsWdIROVc/VVjjkLRJ0sY0yybg0ErOPQRYHl2+ux2YDoxM3cHMVpjZIqCimaPPBZ6KZiB0rkKjR8PGjTBrVtKROFd/VZg4zKytmR2QZmlrZpXVVroCH6asl0Rl2Tof+HOZsqmSFkn6g6Tm6Q6SNKn0Tve1a9dW4WVdXXT66dC2rTdXORen6vRxxE5SF6Af8HRK8TVAH2Aw0AG4Ot2xZjbNzArMrKBz586xx+pyQ4sWMHw4zJgBu3YlHY1z9VOciWM10D1lvVtUlo3zCNPV7igtMLM10UCLXxKGPRlS7UhdvVJYCGvXwssvJx2Jc/VTnIljLtBbUi9JzQhNTjOzPMdYyjRTRbUQJIkwP8hb1Q/V1SdnnQXNm3tzlXNxiS1xmNlO4HuEZqa3gQfNbImk6yWNAJA0WFIJMAa4TdKS0uMl5RFqLC+UOXWxpMXAYqAT8Ku43oOrm9q0ga9/PUzuZD7dmHM1rjqX41bKzJ4EnixT9ouU53MJTVjpjl1Bms50M/M71l2lRo+GmTNhwQIYNCjpaJyrX3K6c9y5qvrGN6BxY2+uci4OnjhcvdSxI5xyiicO5+LgicPVW4WFsGwZvP120pE4V7944nD11qhR4dFrHc7VLE8crt469FA49lhPHM7VNE8crl4rLAxXVq1cmXQkztUfnjhcvTZ6dHh89NFk43CuPvHE4eq1r34V+vf35irnapInDlfvFRbCSy/Bv/6VdCTO1Q+eOFy9V1gYhh557LGkI3GufvDE4eq9o46Cr3zFm6ucqymeOFy9J4Vax6xZsH590tE4V/d54nANQmEh7NgBTzyRdCTO1X2eOFyDMGRIuCHQm6ucqz5PHK5BaNQo3NPx1FOwZUvS0ThXt3nicA1GYSFs3QpPP135vs658nnicA3GSSdBhw7eXOVcdXnicA1GkyYwYgQ8/jhs3550NM7VXbEmDknDJL0jabmkyWm2nyRpgaSdks4ts22XpIXRMjOlvJek16NzPiCpWZzvwdUvhYXhktzZs5OOxLm6K7bEIakxcAtwFtAXGCupb5ndVgETgPvTnGKrmQ2MlhEp5b8F/mBmXwU+BybWePCu3jrzTGjd2purnKuOOGscQ4DlZva+mW0HpgMjU3cwsxVmtgjYnckJJQk4DXgoKroHGFVjEbt6r0ULGD4cZsyAXbuSjsa5uinOxNEV+DBlvSQqy1QLSfMkvSZpVFTWEVhvZjsrO6ekSdHx89auXZtl6K4+Gz06DHj46qtJR+Jc3ZTLneM9zawAuAC4SdJXsjnYzKaZWYGZFXTu3DmeCF2ddPbZ0KyZz9HhXFXFmThWA91T1rtFZRkxs9XR4/vAbCAfWAe0l9SkKud0DuCAA0JfxyOPhFFznXPZiTNxzAV6R1dBNQPOB2ZWcgwAkg6U1Dx63gkYCiw1MwOeB0qvwLoI8MGyXdYKC2HFCli4MOlInKt7YkscUT/E94CngbeBB81siaTrJY0AkDRYUgkwBrhN0pLo8COBeZLeJCSK35jZ0mjb1cCVkpYT+jzujOs9uPprxIgwDIlfXeVc9mQNoK5eUFBg8+bNSzoMl2NOOy10ki9ZUvm+zjVEkuZHfc37yOXOcediNXo0LF0K77yTdCTO1S2eOFyDNWpUePSrq5zLjicO12B17x7m6fB+Duey44nDNWiFhTB3LqxalXQkztUdnjhcgzZ6dHicMSPRMJyrUzxxuAbt8MPhqKO8ucq5bHjicA3e6NHw4ovgQ5o5lxlPHK7BKyyE3bthZkbjGjjnPHG4Bm/AAOjVy5urnMuUJw7X4Emh1vHss7BhQ9LROJf7PHE4R0gc27fDk08mHYlzuc8Th3PAscfCIYdk1lxVXAx5eWGQxLy8sO5cQ+KJwzlCEhg1KtQ4tm4tf7/iYpg0CVauDHN5rFwZ1j15uIbEE4dzkcJC2LIF/v738veZMiXsk2rLllDuXEPhicO5yCmnQPv2FTdXlTc0iQ9Z4hoSTxzORZo2DRM8zZwJO3ak36dHj+zKnauPPHE4l6KwED7/HF54If32qVOhVat9y1q1CuXONRSxJg5JwyS9I2m5pMlptp8kaYGknZLOTSkfKOlVSUskLZL0rZRtd0v6QNLCaBkY53twDcu//VtIBOU1V40bB9OmQc+e4f6Pnj3D+rhxtRunc0mKbepYSY2BfwJnAiXAXGBsytzhSMoDDgB+DMw0s4ei8sMBM7N3JR0KzAeONLP1ku4GHi/dNxM+dazLxrnnwiuvQElJuNrKuYYqialjhwDLzex9M9sOTAdGpu5gZivMbBGwu0z5P83s3ej5R8AnQOcYY3Vuj8JCWLMGXn896Uicy01xJo6uwIcp6yVRWVYkDQGaAe+lFE+NmrD+IKl59cJ0bl/Dh4eOch+7yrn0croiLqkLcB9wsZmV1kquAfoAg4EOwNXlHDtJ0jxJ89b6eNkuC+3awRlnhMQRU0uuc3VanIljNdA9Zb1bVJYRSQcATwBTzOy10nIzW2PBl8BdhCax/ZjZNDMrMLOCzp29lctlp7AQ3n8fFi1KOhLnck+ciWMu0FtSL0nNgPOBjGY8iPZ/FLi3bCd4VAtBkoBRwFs1GbRzEO7naNTIm6ucSye2xGFmO4HvAU8DbwMPmtkSSddLGgEgabCkEmAMcJukJdHh5wEnARPSXHZbLGkxsBjoBPwqrvfgGq6DDoITTvDE4Vw6sV2Om0v8clxXFf/5n/CjH8E//wm9eycdjXO1L4nLcZ2r00aPDo+PPppsHM7lGk8czpWjRw8oKPDmKufK8sThXAVGjw43ApaUJB2Jc7nDE4dzFSgsDI8zZiQahnM5xROHcxXo0weOPNKbq5xL5YnDuUoUFsKcOfDpp0lH4lxu8MThXCUKC2HXLvjrX5OOxLnc4InDuUrk54d5N7y5yrnAE4dzlZDC1VXPPAObNiUdjXPJ88ThXAYKC2H7dnjyyaQjcS55njicy8Dxx4fxq/wucleZnTth/XrYvbvSXeusJkkH4Fxd0LgxjBoF998P27ZBixZJR+TitHs3bNgAn38els8+S/883bbS5symTaF799A/1qPH/kv37tC6dbLvs6o8cTiXocJCmDYNnn0Wzjkn6WhcZcxg8+bKv+jTPV+/vuJJvJo3hwMPDEuHDtCtG/Trt3e9TRv45BNYtSoszz0HH320fy2kY8d9k0nZJHPwwbk5770nDucydOqpYXbARx7xxJFLPv8cXnkFXnoJXnstfEF/9ln48t+5s/zjmjTZ++V/4IGhKfKII/Z++aduK7vesmW4aCIbO3aE2EqTSeqyfHlILps373tMaa0lXY2ldEmi1uKJw7kMNWsWEsZjj4UvpCb+v6fWmcEHH4Qk8fLL4XHp0rCtSZNw6XR+fvlf+Knrbdpk/+WfieJimDIlJIQePWDqVBg3LiSBnj3DUt5727AhfWJZtQpmzcqs1lJ2OeSQmq+1+D9957JQWBi+GObMgdNOSzqa+m/HDli4cG+iePll+PjjsK1dOzjuOLjgAhg6FIYMgVatEg2X4mKYNAm2bAnrK1eGdQjJoyIStG8flv790+9TUa3lvfdCcil7yfiiRaEZrSb5RE7OZeGLL6BzZ5g4Ef7rv5KOpv7ZsAFefXVvknj99b1fwnl5IUEMHRpmZ+zbN1y0kEvy8kKyKKtnT1ixonZi2LABbr0Vfv97WLs2NHX93/9beeJKp7yJnDxxOJelwkL4xz/Cr7xc7LisK8zCZ5ham1i8OJQ3agQDB4YEUZosunZNOuLKNWqUvlNdqr3Lc8vWeiDUxKZNyz55JDIDoKRhkt6RtFzS5DTbT5K0QNJOSeeW2XaRpHej5aKU8kGSFkfnvFmKo5XSufIVFsLq1TB3btKR1C07d8KCBaGmdv754ZdwXh5ceCHcd1+4gujaa8NVaxs2wPz5Yfre886rG0kDQp9CNuVxmDJl36QBYX3KlJp7jdj6OCQ1Bm4BzgRKgLmSZprZ0pTdVgETgB+XObYDcC1QABgwPzr2c+BW4FLgdeBJYBjwVFzvw7myhg8PHbGPPALHHJN0NLlr06bQ1FRao3jttb1XDXXrBieeuLc20a9f/bjYYOrU9L/2p06tvRhWrcquvCri/FMNAZab2fsAkqYDI4E9icPMVkTbylbivg783cw+i7b/HRgmaTZwgJm9FpXfC4zCE4erRQceGDrG//xn6NVrb4dm2aWh3SRYUrK3yemll+DNN0PzjBQ6e8eP39v0VJu/wGtTaVNQuquqakuPHun7WWryM48zcXQFPkxZLwEy/X2W7tiu0VKSpnw/kiYBkwB61Nd/pS4xEyaEL4PLLy9/n2bNyk8q7duHq4Iq2l6VewUytXt36OjfvLl6S+k5Nm2CjRvDuVu1gmOPDV+eQ4eG5+3axfM+UpV3GWxtGzcumdctVRu1nnpQOUzPzKYB0yB0jiccjqtnxo4NQ5CsX59+2bAhffnKlXu3b9tW8Ws0aZJZ4mnWLLMv99SlbBt4RRo3Dvc8lF26dNn7vHVrOOywkCgGDAj3LNSm6lwGW9/URq0nzsSxGuiest4tKsv02FPKHDs7Ku9WxXM6V6NatgxLly5VO37btvITTHnlq1fvfb516/7nbNEi/Zf8QQelL09dWrdOX968eXw1n5pSUYdwQ0scEH+tJ87EMRfoLakX4cv9fOCCDI99Gvi1pAOj9X8DrjGzzyRtlHQsoXN8POBX07s6qUWLsBx8cPbHFhfDpZfumzxatoTbb6/9L8pcaCKqjQ5ht1dsl+Oa2U7ge4Qk8DbwoJktkXS9pBEAkgZLKgHGALdJWhId+xnwS0LymQtcX9pRDvw7cAewHHgP7xh3DdCUKfvXOLZurdlLLjNR2kS0cmW4f6G0iai4uHbjyIXLYBsSvwHQuTooF240g9y4Uxpq9qY3t1ciNwA65+KRK7+wc6WJaNy4kCR69gzJs2dPTxpx8sThXB00der+A/rV9o1mkDsJDEKSWLEi1LhWrPCkESdPHM7VQbnyCztXEpirXfX2Pg7n6rukbzQrjQGSv6rK1S5PHM65asmFBOZqlzdVOeecy4onDuecc1nxxOGccy4rnjicc85lxROHc865rDSIIUckrQXSDIxQp3QCPk06iBzhn8W+/PPYl38ee1X3s+hpZp3LFjaIxFEfSJqXbsyYhsg/i33557Ev/zz2iuuz8KYq55xzWfHE4ZxzLiueOOqOaUkHkEP8s9iXfx778s9jr1g+C+/jcM45lxWvcTjnnMuKJw7nnHNZ8cSRwyR1l/S8pKWSlkj6YdIx5QJJjSW9IenxpGNJmqT2kh6StEzS25KOSzqmpEi6Ivp/8pakP0tqkXRMtUnSnyR9IumtlLIOkv4u6d3o8cCaeC1PHLltJ/AfZtYXOBb4rqS+CceUC34IvJ10EDniP4G/mVkfYAAN9HOR1BX4AVBgZkcBjYHzk42q1t0NDCtTNhl4zsx6A89F69XmiSOHmdkaM1sQPd9E+FLommxUyZLUDRgO3JF0LEmT1A44CbgTwMy2m9n6RINKVhOgpaQmQCvgo4TjqVVmNgf4rEzxSOCe6Pk9wKiaeC1PHHWEpDwgH3g94VCSdhPwE2B3wnHkgl7AWuCuqOnuDkmtkw4qCWa2GrgRWAWsATaY2TPJRpUTDjazNdHzj4GDa+KknjjqAEltgIeBH5nZxqTjSYqkc4BPzGx+0rHkiCbA0cCtZpYPfEENNUXUNVHb/UhCMj0UaC3pwmSjyi0W7r2okfsvPHHkOElNCUmj2MweSTqehA0FRkhaAUwHTpP0v8mGlKgSoMTMSmuhDxESSUN0BvCBma01sx3AI8DxCceUC/4lqQtA9PhJTZzUE0cOkyRC+/XbZvb7pONJmpldY2bdzCyP0PE5y8wa7K9KM/sY+FDSEVHR6cDSBENK0irgWEmtov83p9NALxQoYyZwUfT8IuCxmjipJ47cNhT4NuGX9cJoOTvpoFxO+T5QLGkRMBD4dbLhJCOqdT0ELAAWE77bGtTQI5L+DLwKHCGpRNJE4DfAmZLeJdTKflMjr+VDjjjnnMuG1zicc85lxROHc865rHjicM45lxVPHM4557LiicM551xWPHE4V0WSdqVcJr1QUo3dtS0pL3WUU+dySZOkA3CuDttqZgOTDsK52uY1DudqmKQVkn4nabGkf0j6alSeJ2mWpEWSnpPUIyo/WNKjkt6MltKhMhpLuj2aY+IZSS2j/X8QzdGySNL0hN6ma8A8cThXdS3LNFV9K2XbBjPrB/w3YURfgP8C7jGz/kAxcHNUfjPwgpkNIIw1tSQq7w3cYmZfA9YD34zKJwP50Xkui+etOVc+v3PcuSqStNnM2qQpXwGcZmbvR4NUfmxmHSV9CnQxsx1R+Roz6yRpLdDNzL5MOUce8PdoAh4kXQ00NbNfSfobsBmYAcwws80xv1Xn9uE1DufiYeU8z8aXKc93sbdPcjhwC6F2MjeauMi5WuOJw7l4fCvl8dXo+Svsnc50HPBi9Pw54HLYM596u/JOKqkR0N3MngeuBtoB+9V6nIuT/1JxrupaSlqYsv43Myu9JPfAaMTaL4GxUdn3CbP1XUWYue/iqPyHwLRoNNNdhCSyhvQaA/8bJRcBNzfw6WJdAryPw7kaFvVxFJjZp0nH4lwcvKnKOedcVrzG4ZxzLite43DOOZcVTxzOOeey4onDOedcVjxxOOecy4onDuecc1n5/1xUPxTbH0o9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간헐적으로 학습이 안되며 accuracy 역시 0.35에 수렴하는 문제가 발생하여 to_category를 사용해 주었으나 accuracy가 0.35에서 학습이 또다시 멈추었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 총평\n",
    "\n",
    "가장 높은 정확도는 voting에서 word_num이 None일때 0.81로 가장 높았으나 logistic regression에서 word_num이 none일 때와 소수점 2자리 수에서 근소한 차이를 보이는 정도라 코스트를 고려하였을때 가장 효율적인 것은 logistic regression(word_num = None)으로 결론낼 수 있었다.\n",
    "\n",
    "RNN모델과 비교를 시도하였으나 모델 설계 오류로 인해 정상적인 성능을 갖추지 못하여 직접적으로 비교할 수 없었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
